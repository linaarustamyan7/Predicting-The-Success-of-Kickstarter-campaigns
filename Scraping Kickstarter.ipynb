{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re \n",
    "import numpy as np\n",
    "from scrapy.http import TextResponse\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape the urls from the Kicktraq's main page\n",
    "def urlscraper(url,base_url=\"https://www.kicktraq.com\"):\n",
    "  page = requests.get(url)\n",
    "  response = TextResponse(body=page.text,url=url,encoding=\"utf-8\")\n",
    "  url = [base_url + i for i in response.css(\"div[class = 'project-infobox'] h2>a::attr(href)\").extract()]\n",
    "  return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collecting the urls of all projects available available\n",
    "pages = []\n",
    "for i in range(0,700):\n",
    "  pages.extend(urlscraper(url = f\"https://www.kicktraq.com/archive/?page={i}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the list of urls to DataFrame\n",
    "df = pd.DataFrame({\"URL\":pages})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all the available features from individual projects pages in Kicktraq\n",
    "def kicktraqscraper(url):\n",
    "    page = requests.get(url,verify=False)\n",
    "    response = TextResponse(body=page.text,url=url,encoding=\"utf-8\")\n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    '''\n",
    "    Some project sites don't provide data on average pledge per backer and the order of information changes, so why I use if else \n",
    "    statements to correctly scrape all the needed data \n",
    "    '''\n",
    "    info = response.css(\"div#project-info-text::text\").extract()\n",
    "    res = [i for i in info if \"Average Pledge Per Backer\" in i]\n",
    "    if len(res) > 0:\n",
    "      back = response.css(\"div#project-info-text::text\").extract()[2]\n",
    "      backer = [int(s) for s in back.split() if s.isdigit()][0]\n",
    "      pl = response.css(\"div#project-info-text::text\").extract()[3]\n",
    "      pled = regex.sub(\"\", pl)\n",
    "      pledge_per_backer = pled.split(\": \")[1]\n",
    "      fund = response.css(\"div#project-info-text::text\").extract()[5]\n",
    "      money = regex.sub(\"\", fund)\n",
    "      money = money.split(\": \")[1]\n",
    "      funded = money.split(\" of \")[0]\n",
    "      goal =  money.split(\" of \")[1]\n",
    "    else:\n",
    "      back = response.css(\"div#project-info-text::text\").extract()[2]\n",
    "      backer = [int(s) for s in back.split() if s.isdigit()][0]\n",
    "      pledge_per_backer = \"None\"\n",
    "      fund = response.css(\"div#project-info-text::text\").extract()[4]\n",
    "      money = regex.sub(\"\", fund)\n",
    "      money = money.split(\": \")[1]\n",
    "      funded = money.split(\" of \")[0]\n",
    "      goal =  money.split(\" of \")[1]\n",
    "        \n",
    "    status = response.css(\"div[class = 'ribbon'] h3::text\").extract_first(default = None)\n",
    "    url =  response.css(\"div[id = 'project-info-image'] a::attr(href)\").extract_first(default = None)\n",
    "    creator = response.css(\"div[id = 'project-info-text'] a[target='_blank']::attr(href)\").extract_first(default = None)\n",
    "    category = response.css(\"div[class='project-cat']>a::text\").extract_first(default = None)\n",
    "    start_date = response.css(\"div[id = 'project-info-text'] a[class = 'datelink']::text\").extract()[0]\n",
    "    end_date =  response.css(\"div[id = 'project-info-text'] a[class = 'datelink']::text\").extract()[1]\n",
    "    desc = response.css(\"div#project-info-text::text\").extract_first(default = None)\n",
    "    description = regex.sub(\"\", desc)\n",
    "    return {\"status\":status, \"url\":url,\"creator\":creator, \"category\":category,\"start date\":start_date,\"end date\":end_date,\"description\":description,\"backer\":backer,\"pledge_per_backer\":pledge_per_backer, \"funded\":funded,\"goal\":goal}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the features by going to all individual projects by urls we have already scraped\n",
    "traqdata = []\n",
    "for i in df.URL.to_list():\n",
    "  info = kicktraqscraper(url = i)\n",
    "  traqdata.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming to DataFrame to be able to download\n",
    "data_kicktraq = pd.DataFrame.from_dict(traqdata)\n",
    "data_kicktraq.to_csv(\"kicktraq_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Redirecting to the page of a project in Kickstarter by the link provided in Kicktraq page and scraping the additional features that were not available in Kicktraq.\n",
    "def kickstarterscraper(url):\n",
    "    page = requests.get(url,verify = False)\n",
    "    response = TextResponse(body=page.text,url=url,encoding=\"utf-8\")\n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    '''\n",
    "    As in different pages some features project  were not in the  same section \n",
    "    I generated if else statements  to scrape for all.\n",
    "    '''\n",
    "    title_other = response.css(\"a.hero__link::text\").extract_first(default = None)\n",
    "    title = response.css(\"div[class = 'grid-col-10 grid-col-10-lg grid-col-offset-1-md block-md order2-md type-center'] h2::text\").extract_first(default = None)\n",
    "    if title:\n",
    "        title = title  \n",
    "    elif title_other:\n",
    "        title = title_other\n",
    "    else:\n",
    "        title = \"\"\n",
    "    description_other = response.css(\"span.relative>span[class ='content edit-profile-blurb js-edit-profile-blurb']::text\").extract_first(default = None)\n",
    "    description = response.css(\"div[class = 'grid-col-10 grid-col-10-lg grid-col-offset-1-md block-md order2-md type-center'] p[class='type-14 type-18-md soft-black project-description mb1']::text\").extract_first(default = None)\n",
    "    if description:\n",
    "        descr = description\n",
    "        len_desc = len(description)\n",
    "    elif description_other:\n",
    "        descr = description_other\n",
    "        len_desc = len(description_other)\n",
    "    else:\n",
    "        descr = \"\"\n",
    "        len_desc = \"\"\n",
    "    featured = response.css(\"span.ml1>span::text\").extract_first(default = None)\n",
    "    featured_other = response.css(\"div[class='NS_projects__category_location ratio-16-9 flex items-center'] a[class='grey-dark mr3 nowrap type-12 flex items-center']::text\").extract()\n",
    "    if featured and featured == 'Project We Love':\n",
    "        fea = 1 #Featured\n",
    "    elif featured_other and len(featured_other)>0:\n",
    "        fea = 1 #Featured\n",
    "    else:\n",
    "        fea = 0 # Not featured\n",
    "    \n",
    "    update = response.css(\"a[class ='js-analytics-section js-load-project-content js-load-project-updates mx3 project-nav__link--updates tabbed-nav__link type-14'] span[class = 'count']::text\").extract_first()\n",
    "    comment = response.css(\"a[class ='js-analytics-section js-load-project-comments js-load-project-content mx3 project-nav__link--comments tabbed-nav__link type-14'] span[class = 'count']>data[itemprop='Project[comments_count]']::text\").extract_first()\n",
    "    faq = response.css(\"a[class ='js-analytics-section js-load-project-content js-load-project-faqs mx3 project-nav__link--faqs tabbed-nav__link type-14'] span[class = 'count']::text\").extract_first()\n",
    "    \n",
    "    location = response.css(\"a[class ='nowrap navy-700 flex items-center medium mr3 type-12 keyboard-focusable'] span[class = 'ml1']::text\").extract_first(default = None)\n",
    "    location_other = response.css(\"a[class = 'grey-dark mr3 nowrap type-12']::text\").extract_first(default = None)\n",
    "    if location:\n",
    "        loc = location \n",
    "    elif location_other:\n",
    "        loc = location_other\n",
    "    else:\n",
    "        loc = \"\"\n",
    "# Scraping number of backers per tier\n",
    "    b_p_t = response.css(\"div[class = 'text-nowrap type-12 support-500 radius6px px2 py4px bg-support-100']::text\").extract()\n",
    "# Cleaning the data by keeping only numeric charachters\n",
    "    backer_per_tier = list(map(lambda sub:int(''.join([i for i in sub if i.isnumeric()])), b_p_t))\n",
    "# If a project have reward tiers scrape the number of tiers and average number of backers per tier.\n",
    "    tier = len(backer_per_tier)\n",
    "    if len(backer_per_tier)>0:\n",
    "        avg_backer_per_tier = sum(backer_per_tier) / len(backer_per_tier)\n",
    "        num_pledge_backers = sum(backer_per_tier)\n",
    "    else:\n",
    "        avg_backer_per_tier = \"None\"\n",
    "        num_pledge_backers = \"None\"\n",
    "    # Some project don't have reward tiers   \n",
    "    pledge_lim = response.css(\"h2>span.money::text\").extract()\n",
    "    tier_limits = list(map(lambda sub:int(''.join([i for i in sub if i.isnumeric()])), pledge_lim))\n",
    "    if len(tier_limits)>0:\n",
    "        mean_limit =  sum(tier_limits) / len(tier_limits)\n",
    "        min_limit = min(tier_limits)\n",
    "        max_limit = max(tier_limits)\n",
    "    else:\n",
    "        mean_limit =  \"None\"\n",
    "        min_limit = \"None\"\n",
    "        max_limit = \"None\"\n",
    "    return {\"title\":title,\"description\":descr,\"len_desc\":len_desc,\"num_update\":update, \"num_comment\":comment,\"num_faq\":faq, \"location\":loc,\"num_tiers\":tier,\"num_pledge_backers\":num_pledge_backers,\"backer_per_tier\":avg_backer_per_tier,\"featured\":fea,\"tier_limits\":tier_limits,\"min_limit\":min_limit,\"max_limit\":max_limit,\"mean_limit\":mean_limit}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runing the above mentioned function for all projects we have \n",
    "kickdata = []\n",
    "for i in data_kicktraq.url.to_list():\n",
    "  info = kickstarterscraper(url = i)\n",
    "  kickdata.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming to Dataframe and downloading\n",
    "data_kickstarter = pd.DataFrame.from_dict(kickdata)\n",
    "data_kickstarter.to_csv(\"kikstarter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping profile of  creators by redirecting to their profile page in Kickstarter\n",
    "def creatorbioscraper(url):\n",
    "    page = requests.get(url,verify = False)\n",
    "    response = TextResponse(body=page.text,url=url,encoding=\"utf-8\")\n",
    "    info = response.css(\"div[class = 'created-projects py2 f5 mb3'] ::text\").extract()\n",
    "    info.remove('Â·')\n",
    "    info = ' '.join(info).split()\n",
    "    created = info[0]\n",
    "    # Number of backed projects by creator\n",
    "    backed = int(info[2])\n",
    "    # Number of created projects by creator \n",
    "    if \"First\" in created:\n",
    "        creat = 1\n",
    "    else:\n",
    "        creat = int(created)\n",
    "    web = response.css(\"div[class = 'pt3 pt7-sm mobile-hide'] ::text\").extract()\n",
    "    if len(web)>0:\n",
    "        website = 1 #Have website(s)\n",
    "    else:\n",
    "        website = 0 # Dont have website(s)\n",
    "    fb = response.css(\"div[class = 'facebook py2 border-bottom f5'] ::text\").extract()\n",
    "    fb = ' '.join(fb).split()\n",
    "    if \"Not\" in fb:\n",
    "        fb = 0 #Don't connected facebook\n",
    "    else:\n",
    "        fb = 1 # Connected to facebook\n",
    "    # Creator's name\n",
    "    creator = response.css(\"span.identity_name::text\").extract_first(default = None)\n",
    "    regex = re.compile(r'[\\n\\r\\t]')\n",
    "    creator = regex.sub(\"\", creator)\n",
    "    # Some details about creator\n",
    "    about = response.css(\"div.readability>p::text\").extract_first(default = None)\n",
    "    if about:\n",
    "        len_about = len(about)\n",
    "    else:\n",
    "        len_about = 0\n",
    "    return {\"num_backed_by_owner\":backed,\"num_created_by_owner\":creat,\"website\":website,\"facebook\":fb,\"owner\":creator,\"about\":about,\"len_about\":len_about}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runing the function above for all projects\n",
    "creatordata =[]\n",
    "for i in data_kicktraq.creator.to_list():\n",
    "  info = creatorbioscraper(url = i)\n",
    "  creatordata.append(info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data \n",
    "data_creator = pd.DataFrame.from_dict(creatordata)\n",
    "data_creator.to_csv(\"creatordata_thirdset_half.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
